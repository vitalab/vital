{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "import holoviews as hv\n",
    "import matplotlib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import panel as pn\n",
    "import seaborn as sns\n",
    "from dotenv import load_dotenv\n",
    "from matplotlib import pyplot as plt\n",
    "from scipy import stats\n",
    "\n",
    "from vital.data.cardinal.config import TabularAttribute\n",
    "from vital.data.cardinal.config import View as ViewEnum\n",
    "from vital.data.cardinal.records2yaml import read_records\n",
    "from vital.data.cardinal.utils.itertools import Patients\n",
    "\n",
    "hv.extension(\"bokeh\")\n",
    "\n",
    "load_dotenv()  # Load environment variables from `.env` file if it exists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the patient records from a CSV file or"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "records_csv = \"~/dataset/cardinal/hcl/patient_records.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "csv_records = read_records(records_csv)\n",
    "\n",
    "csv_records"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the patient attributes from the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "data_roots = [Path(os.environ[\"CARDINAL_DATA_PATH\"])]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (Optional) Hard-coded lists of patients that we might want to discard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "\n",
    "\n",
    "def load_list_from_file(filepath: str | Path) -> List[str]:\n",
    "    return Path(filepath).read_text().splitlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_patients = [\"0063\"]\n",
    "unusable_masks = [\"0119\", \"0126\", \"0135\", \"0147\", \"0153\", \"0158\", \"0165\", \"0228\"]\n",
    "\n",
    "exclude_patients = missing_patients + unusable_masks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (Optional) Hard-coded lists of patients that we want to choose from"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clustering_results_path = Path(\"~/data/didactic/results/cardiac_multimodal_representation_clustering\").expanduser()\n",
    "model = \"\"\n",
    "patients_by_cluster = [load_list_from_file(cluster_file) for cluster_file in sorted((clustering_results_path / f\"{model}-diag\").glob(\"*.txt\"))]\n",
    "\n",
    "include_patients = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the patients attributes using the custom collections API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "patients_filter_kwargs = {'include_patients': include_patients} if include_patients is not None else {'exclude_patients': exclude_patients}\n",
    "patients = Patients(data_roots, views=[ViewEnum.A4C, ViewEnum.A2C], **patients_filter_kwargs)\n",
    "\n",
    "dataset_records = patients.to_dataframe()\n",
    "dataset_records"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Select which records (from the CSV or from the dataset) to use for further analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "records = dataset_records\n",
    "\n",
    "records"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute statistics on the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Differentiate between numerical and categorical attributes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_variables = TabularAttribute.categorical_attrs()\n",
    "numerical_variables = TabularAttribute.numerical_attrs()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Describe numerical variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "with pd.option_context(\"display.max_columns\", None):\n",
    "    display(records[numerical_variables].describe().drop([\"count\"]).round(decimals=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Describe categorical variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "with pd.option_context(\"display.max_columns\", None):\n",
    "    display(records[categorical_variables].describe().drop([\"count\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lists variables, in descending order of missing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "missing_data_by_attr = pd.DataFrame(records.isna().sum(axis=\"index\"), columns=[\"num\"])\n",
    "missing_data_by_attr[\"%\"] = missing_data_by_attr.num * 100 / len(records)\n",
    "missing_data_by_attr = missing_data_by_attr.sort_values(ascending=False, by=\"num\").round(decimals=1)\n",
    "\n",
    "with pd.option_context(\"display.max_rows\", None, \"display.max_columns\", None):\n",
    "    display(missing_data_by_attr[missing_data_by_attr[\"%\"] > 0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Count patients with all the requested data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dynamically determine which column to discard based on availability of data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# ignore_cols = []\n",
    "ignore_cols = missing_data_by_attr[missing_data_by_attr[\"%\"] > 10].index.tolist()\n",
    "# ignore_cols = [\"la_area\", \"vmax_tr\", \"s_prime\", \"tapse\", \"dbp_tte\", \"sbp_tte\", \"dpb_day\", \"sbp_day\", \"sbp_night\", \"dpb_night\", \"septal_e_prime\", \"lateral_e_prime\"]\n",
    "ignore_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Discard selected columns\n",
    "cols = records.columns.difference(ignore_cols)\n",
    "records_subset = records[cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for data_tag, data in [(\"all data\", records), (\"subset of variables\", records_subset)]:\n",
    "    nb_remaining_patients = data.notna().all(axis=\"columns\").sum()\n",
    "    print(f\"Nb patients w/ full data when using {data_tag}: {nb_remaining_patients}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analyze distribution of variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Analyze distribution of numerical variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_attr_select = pn.widgets.Select(name=\"Numerical attribute\", value=numerical_variables[0], options=numerical_variables)\n",
    "\n",
    "@pn.depends(num_attr=num_attr_select, watch=True)\n",
    "def _update_distribution(num_attr: str) -> hv.Distribution:\n",
    "    # Cast numerical dtypes to float to avoid problems when computing a distribution from integer columns\n",
    "    return hv.Distribution(records[num_attr].astype(float))\n",
    "\n",
    "pn.Row(pn.Column(num_attr_select), hv.DynamicMap(_update_distribution).opts(width=600, height=600, framewise=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Analyze distribution of categorical variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for cat_attr in categorical_variables:\n",
    "    print(cat_attr)\n",
    "    cat_attr_data = records[cat_attr]\n",
    "    \n",
    "    labels = cat_attr_data.unique()\n",
    "    labels_stats = {label: (label_count := (cat_attr_data == label).sum(), label_count * 100 / len(cat_attr_data)) for label in labels}\n",
    "    \n",
    "    for label, label_stats in labels_stats.items():\n",
    "        print(f\"{label}: {label_stats[0]} ({label_stats[1]:.1f}%)\")\n",
    "    \n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Analyze relationships between variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_attr_1_select = pn.widgets.Select(name=\"Numerical attribute (x)\", value=numerical_variables[0], options=numerical_variables)\n",
    "num_attr_2_select = pn.widgets.Select(name=\"Numerical attribute (y)\", value=numerical_variables[1], options=numerical_variables)\n",
    "cat_attr_select = pn.widgets.Select(name=\"Categorical attribute (color)\", value=categorical_variables[0], options=categorical_variables)\n",
    "\n",
    "@pn.depends(num_attr=num_attr_1_select, watch=True)\n",
    "def _update_x_dist(num_attr: str) -> hv.Distribution:\n",
    "    # Cast numerical dtypes to float to avoid problems when computing a distribution from integer columns\n",
    "    return hv.Distribution(records[num_attr].astype(float))\n",
    "\n",
    "@pn.depends(num_attr=num_attr_2_select, watch=True)\n",
    "def _update_y_dist(num_attr: str) -> hv.Distribution:\n",
    "    # Cast numerical dtypes to float to avoid problems when computing a distribution from integer columns\n",
    "    return hv.Distribution(records[num_attr].astype(float))\n",
    "\n",
    "@pn.depends(num_attr_1=num_attr_1_select, num_attr_2=num_attr_2_select, cat_attr=cat_attr_select, watch=True)\n",
    "def _update_points(num_attr_1: str, num_attr_2: str, cat_attr: str) -> hv.Points:\n",
    "    # Cast numerical dtypes to float to avoid problems with serializing pd.NaT (missing values of pandas' integer dtypes) \n",
    "    # when processing the data for the the scatter plot\n",
    "    points_data = records[[num_attr_1, num_attr_2, cat_attr]].astype({num_attr_1: float, num_attr_2: float})\n",
    "    return hv.Points(points_data, kdims=[num_attr_1, num_attr_2], vdims=[cat_attr]).opts(color=cat_attr, cmap=\"Set1\")\n",
    "\n",
    "widgets_layout = pn.Column(num_attr_1_select, num_attr_2_select, cat_attr_select)\n",
    "plots_layout = (\n",
    "    hv.DynamicMap(_update_points).opts(width=600, height=600, framewise=True, size=4) <<\n",
    "    hv.DynamicMap(_update_y_dist).opts(width=150, framewise=True) <<\n",
    "    hv.DynamicMap(_update_x_dist).opts(height=150, framewise=True)\n",
    ")\n",
    "\n",
    "pn.Row(widgets_layout, plots_layout)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Correlation between attributes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Encode categorical attributes as numerical attributes, to give option to include them when comparing target to numerical attributes\n",
    "\n",
    "The categorical attributes are encoded as the mean/median of the target attribute w.r.t. each class.\n",
    "\n",
    "However, this could possibly overestimate the correlation of categorical attributes w.r.t. target attribute (e.g. the mean/median by class is coincidentaly correlated with the target attribute, even if the variance within each class is as high as the global variance). Therefore, this measure is presented to provide a comparable perspective between categorical and numerical attributes, but it should be interpreted cautiously."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the function that encodes the categorical attributes as stats of the target attribute w.r.t. each class\n",
    "\n",
    "def encode_cat_attr(records: pd.DataFrame, target_attr: TabularAttribute, cat_attr: TabularAttribute, stat: str = 'mean') -> pd.Series:\n",
    "    target_vals = records[[cat_attr, target_attr]].groupby(cat_attr)\n",
    "    target_stats = getattr(target_vals, stat)()[target_attr]\n",
    "    \n",
    "    cat_encodings = pd.Series(index=records.index)\n",
    "    for label, target_stat in target_stats.items():\n",
    "        cat_encodings.loc[records[cat_attr] == label] = target_stat\n",
    "    \n",
    "    return cat_encodings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the target attribute and the attributes to encode\n",
    "target = TabularAttribute.sbp_24\n",
    "cat_attrs = TabularAttribute.categorical_attrs()\n",
    "if is_target_categorical := target in cat_attrs:\n",
    "    cat_attrs.remove(target) # Remove target from categorical attrs in case it is one of the attributes that can be converted to numerical values\n",
    "\n",
    "encoded_records = records.copy()\n",
    "if is_target_categorical:\n",
    "    # If target is categorical, convert it to numerical values (assuming it will if attribute is not suitable)\n",
    "    encoded_records[target] = encoded_records[target].astype(int)\n",
    "    # Also encode the target (w.r.t itself) so that it also has a numerical encoding we can use in downstream operations\n",
    "    encoded_records[target + \"_E\"] = encoded_records[target]\n",
    "\n",
    "for cat_attr in cat_attrs:\n",
    "    col_idx = encoded_records.columns.to_list().index(cat_attr)\n",
    "    encoding = encode_cat_attr(encoded_records, target, cat_attr)\n",
    "    encoded_records.insert(col_idx + 1, cat_attr + \"_E\", encoding)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Correlation between categorical attributes (Cramér's V)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the computation of the statistic used to measure correlation between categorical variables\n",
    "\n",
    "def cramers_corrected_stat(confusion_matrix: pd.DataFrame) -> float:\n",
    "    \"\"\"Computes the version of Cramér's V corrected by Bergsma and Wicher for categorial-categorial association.\"\"\"\n",
    "    res = stats.chi2_contingency(confusion_matrix)\n",
    "    n = confusion_matrix.sum().sum() # Sum over both rows and columns, to get the total number of samples\n",
    "    phi2 = res.statistic/n\n",
    "    r, k = confusion_matrix.shape\n",
    "    phi2corr = max(0, phi2 - ((k-1)*(r-1))/(n-1))\n",
    "    rcorr = r - ((r-1)**2)/(n-1)\n",
    "    kcorr = k - ((k-1)**2)/(n-1)\n",
    "    return np.sqrt(phi2corr / min( (kcorr-1), (rcorr-1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "is_executing": true,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Compute the correlation matrix between categorical attributes\n",
    "cramers_v_matrix = pd.DataFrame(index=TabularAttribute.categorical_attrs())\n",
    "\n",
    "# Iterate over attributes as targets and compare the remaining attributes to it\n",
    "for target in TabularAttribute.categorical_attrs():\n",
    "    other_cat_attrs = TabularAttribute.categorical_attrs()\n",
    "    other_cat_attrs.remove(target)\n",
    "\n",
    "    # Compute Cramér's V for other attributes w.r.t. the target\n",
    "    target_corr = {cat_attr: cramers_corrected_stat(pd.crosstab(records[target], records[cat_attr])) for cat_attr in other_cat_attrs}\n",
    "    target_corr[target] = 1 # Set correlation between target and itself to 1\n",
    "    \n",
    "    cramers_v_matrix[target] = pd.Series(data=target_corr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the pairwise similarity matrix between categorical attributes\n",
    "from vital.utils.plot import plot_heatmap\n",
    "\n",
    "plot_heatmap(cramers_v_matrix, annot_kws={\"fontsize\": \"small\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Correlation between categorical attribute and numerical attributes (Kruskal-Wallis H)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define the computation of the test used to measure correlation between categorical and numerical attributes\n",
    "\n",
    "def kruskal(records: pd.DataFrame, target_attr: TabularAttribute, num_attr: TabularAttribute) -> float:\n",
    "    samples = []\n",
    "    for label in records[target_attr].dropna().unique():\n",
    "        label_samples = records[records[target_attr] == label][num_attr].values\n",
    "        label_samples = label_samples.astype(float) # Convert all numerical types to float so that scipy can properly handle NaNs\n",
    "        samples.append(label_samples)\n",
    "    kruskal_res = stats.kruskal(*samples, nan_policy=\"omit\")\n",
    "    return kruskal_res.pvalue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "is_executing": true,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Compute the correlation matrix between categorical attributes\n",
    "kruskal_matrix = pd.DataFrame(index=TabularAttribute.numerical_attrs())\n",
    "\n",
    "# Iterate over attributes as targets and compare the remaining attributes to it\n",
    "for target in TabularAttribute.categorical_attrs():\n",
    "    \n",
    "    # Perform the Kruskal-Wallis H for other attributes w.r.t. the target\n",
    "    target_pvalues = {num_attr: kruskal(encoded_records, target, num_attr) for num_attr in TabularAttribute.numerical_attrs()}\n",
    "    # Uncomment the line below to compare the target to other categorical attributes' using their numerical encodings and the Kruskal-Wallis H test\n",
    "    # target_pvalues.update({cat_attr + \"_E\": kruskal(encoded_records, target, cat_attr + \"_E\") for cat_attr in other_cat_attrs if cat_attr != target})\n",
    "    target_disparity = pd.Series(data={attr: np.log(1 / pvalue) for attr, pvalue in target_pvalues.items()})\n",
    "    \n",
    "    kruskal_matrix[target] = target_disparity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the similarity matrix between categorical and numerical attributes\n",
    "plot_heatmap(kruskal_matrix.T, annot_kws={\"fontsize\": \"small\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Correlation between numerical attributes (encoding + Spearman)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the correlation matrix between numerical attributes\n",
    "num_attrs = TabularAttribute.numerical_attrs()\n",
    "encoded_cat_attrs = [cat_attr + \"_E\" for cat_attr in TabularAttribute.categorical_attrs()]\n",
    "corr_matrix = encoded_records[num_attrs + encoded_cat_attrs].corr(method=\"spearman\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the pairwise similarity matrix between numerical attributes\n",
    "include_encoded_cat_attrs = True\n",
    "\n",
    "variables = num_attrs\n",
    "if include_encoded_cat_attrs:\n",
    "    variables += encoded_cat_attrs\n",
    "    \n",
    "plot_heatmap(corr_matrix.loc[variables, variables], cmap=\"icefire\", annot_kws={\"fontsize\": \"small\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Detailed correlation between target attributes and other attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define generic function to plot rows from similarity matrices as barplot\n",
    "# This allows to more easily inspect correlation between specific attributes of interest to other attributes\n",
    "\n",
    "def similary_matrix_row_barplot(matrix: pd.DataFrame, target: str, similarity_name: str, ascending: bool = True):\n",
    "    # Extract the similarity of the target w.r.t. other attributes from the matrix\n",
    "    plot_data = matrix.reset_index()[[\"index\", target]]\n",
    "    # Exclude similarity w/ itself\n",
    "    plot_data = plot_data[plot_data[\"index\"] != target]\n",
    "    # Sort the values for more easily readable plots\n",
    "    plot_data = plot_data.sort_values(target, ascending=ascending)\n",
    "    \n",
    "    with sns.axes_style(\"darkgrid\"):\n",
    "        # Orient the barplot horizontally so that it scales better w/ more attributes\n",
    "        default_figsize = matplotlib.rcParams['figure.figsize']\n",
    "        fig, ax = plt.subplots(figsize=(default_figsize[0], 0.25 * len(plot_data)))\n",
    "        \n",
    "        ax = sns.barplot(data=plot_data, y=\"index\", x=target, orient=\"h\", ax=ax)\n",
    "        ax.set(ylabel=None, xlabel=similarity_name, title=f\"Similarity between {target} and other attributes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set target attributes for the detailed comparisons in the following cells\n",
    "targets = [\n",
    "    TabularAttribute.nt_probnp_group,\n",
    "    TabularAttribute.ht_severity,\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Correlation with categorical attributes (Cramér's V)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "is_executing": true,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "for target in targets:\n",
    "    ax = similary_matrix_row_barplot(cramers_v_matrix, target, \"Cramér's V\", ascending=False)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "Correlation with numerical attributes (Kruskal-Wallis H)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for target in targets:\n",
    "    ax = similary_matrix_row_barplot(kruskal_matrix, target, \"Disparity from Kruskall-Wallis H test\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "Correlation with all other attributes (encoding + Spearman)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for target in targets:\n",
    "    ax = similary_matrix_row_barplot(corr_matrix, target + \"_E\", \"Spearman Correlation\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
